{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Working with RDDs:\n",
    "   a) Write a Python program to create an RDD from a local data source.\n",
    "   b) Implement transformations and actions on the RDD to perform data processing tasks.\n",
    "   c) Analyze and manipulate data using RDD operations such as map, filter, reduce, or aggregate.\n",
    "\n",
    "2. Spark DataFrame Operations:\n",
    "   a) Write a Python program to load a CSV file into a Spark DataFrame.\n",
    "   b)Perform common DataFrame operations such as filtering, grouping, or joining.\n",
    "   c) Apply Spark SQL queries on the DataFrame to extract insights from the data.\n",
    "\n",
    "3. Spark Streaming:\n",
    "  a) Write a Python program to create a Spark Streaming application.\n",
    "   b) Configure the application to consume data from a streaming source (e.g., Kafka or a socket).\n",
    "   c) Implement streaming transformations and actions to process and analyze the incoming data stream.\n",
    "\n",
    "4. Spark SQL and Data Source Integration:\n",
    "   a) Write a Python program to connect Spark with a relational database (e.g., MySQL, PostgreSQL).\n",
    "   b)Perform SQL operations on the data stored in the database using Spark SQL.\n",
    "   c) Explore the integration capabilities of Spark with other data sources, such as Hadoop Distributed File System (HDFS) or Amazon S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDExample\")\n",
    "\n",
    "# Create an RDD from a local data source (list)\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Print the RDD elements\n",
    "print(rdd.collect())\n",
    "\n",
    "# Terminate the SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDTransformations\")\n",
    "\n",
    "# Create an RDD from a local data source (list)\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply transformations and actions\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)  # Squares each element\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Filters elements greater than 10\n",
    "sum_of_squared = filtered_rdd.reduce(lambda x, y: x + y)  # Calculates the sum of squared elements\n",
    "\n",
    "# Print the results\n",
    "print(squared_rdd.collect())  # [1, 4, 9, 16, 25]\n",
    "print(filtered_rdd.collect())  # [16, 25]\n",
    "print(sum_of_squared)  # 41\n",
    "\n",
    "# Terminate the SparkContext\n",
    "sc.stop()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local\", \"RDDOperations\")\n",
    "\n",
    "# Create an RDD from a local data source (list)\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Perform RDD operations\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)  # Square each element\n",
    "filtered_rdd = squared_rdd.filter(lambda x: x > 10)  # Filter elements greater than 10\n",
    "sum_of_squared = filtered_rdd.reduce(lambda x, y: x + y)  # Sum the filtered elements\n",
    "average = filtered_rdd.aggregate((0, 0),  # Calculate average using aggregate\n",
    "                                 lambda acc, value: (acc[0] + value, acc[1] + 1),\n",
    "                                 lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n",
    "average = average[0] / average[1]\n",
    "\n",
    "# Print the results\n",
    "print(squared_rdd.collect())  # [1, 4, 9, 16, 25]\n",
    "print(filtered_rdd.collect())  # [16, 25]\n",
    "print(sum_of_squared)  # 41\n",
    "print(average)  # 20.5\n",
    "\n",
    "# Terminate the SparkContext\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show()\n",
    "\n",
    "# Terminate the SparkSession\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform DataFrame operations\n",
    "filtered_df = df.filter(df[\"age\"] > 30)  # Filter rows where age is greater than 30\n",
    "grouped_df = df.groupBy(\"gender\").count()  # Group by gender and count occurrences\n",
    "joined_df = df.join(other_df, \"id\", \"inner\")  # Inner join with another DataFrame on \"id\" column\n",
    "\n",
    "# Show the results\n",
    "filtered_df.show()\n",
    "grouped_df.show()\n",
    "joined_df.show()\n",
    "\n",
    "# Terminate the SparkSession\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQLExample\").getOrCreate()\n",
    "\n",
    "# Load a CSV file into a DataFrame\n",
    "df = spark.read.csv(\"path/to/csv/file.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create a temporary view for the DataFrame\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Apply Spark SQL queries\n",
    "result = spark.sql(\"SELECT * FROM people WHERE age > 30\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "# Terminate the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local[2]\", \"StreamingExample\")\n",
    "\n",
    "# Create a StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Configure the application to consume data from a streaming source (e.g., Kafka or a socket)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)  # Example with socket stream, replace with your source\n",
    "\n",
    "# Implement streaming transformations and actions to process and analyze the incoming data stream\n",
    "word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Create a SparkContext\n",
    "sc = SparkContext(\"local[2]\", \"StreamingTransformations\")\n",
    "\n",
    "# Create a StreamingContext with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, 1)\n",
    "\n",
    "# Configure the application to consume data from a streaming source (e.g., Kafka or a socket)\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)  # Example with socket stream, replace with your source\n",
    "\n",
    "# Implement streaming transformations and actions\n",
    "filtered_lines = lines.filter(lambda line: \"error\" in line.lower())  # Filter lines containing \"error\"\n",
    "word_counts = filtered_lines.flatMap(lambda line: line.split(\" \")) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Print the word counts\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the streaming computation\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the computation to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQLDatabase\").getOrCreate()\n",
    "\n",
    "# Connect Spark with a relational database\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Load data from a database table into a DataFrame\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties=db_properties)\n",
    "\n",
    "# Show the DataFrame content\n",
    "df.show()\n",
    "\n",
    "# Terminate the SparkSession\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"SparkSQLDatabaseOperations\").getOrCreate()\n",
    "\n",
    "# Connect Spark with a relational database\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"username\",\n",
    "    \"password\": \"password\"\n",
    "}\n",
    "\n",
    "# Load data from a database table into a DataFrame\n",
    "df = spark.read.jdbc(url=jdbc_url, table=\"mytable\", properties=db_properties)\n",
    "\n",
    "# Perform SQL operations on the DataFrame\n",
    "result = spark.sql(\"SELECT * FROM mytable WHERE age > 30\")\n",
    "\n",
    "# Show the results\n",
    "result.show()\n",
    "\n",
    "# Terminate the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
