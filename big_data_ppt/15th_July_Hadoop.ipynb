{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hadoop_config(config_file):\n",
    "    with open(config_file, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<name>core-site\"):\n",
    "                print(line)\n",
    "            elif line.startswith(\"<name>hdfs-site\"):\n",
    "                print(line)\n",
    "\n",
    "# Usage:\n",
    "config_file = \"hadoop.conf\"\n",
    "read_hadoop_config(config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def get_directory_size(directory):\n",
    "    command = f\"hdfs dfs -du -s {directory}\"\n",
    "    output = subprocess.check_output(command, shell=True).decode(\"utf-8\").strip()\n",
    "    size = int(output.split()[0])\n",
    "    return size\n",
    "\n",
    "# Usage:\n",
    "directory = \"/user/hadoop/data\"\n",
    "total_size = get_directory_size(directory)\n",
    "print(f\"Total file size in {directory}: {total_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_top_words(file_path, n):\n",
    "    words = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            words.extend(line.strip().split())\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    top_words = word_counts.most_common(n)\n",
    "    \n",
    "    for word, count in top_words:\n",
    "        print(f\"{word}: {count}\")\n",
    "\n",
    "# Usage:\n",
    "file_path = \"large_text_file.txt\"\n",
    "n = 10\n",
    "find_top_words(file_path, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def check_hadoop_health(nn_url, dn_url):\n",
    "    nn_status = requests.get(nn_url)\n",
    "    dn_status = requests.get(dn_url)\n",
    "\n",
    "    if nn_status.ok:\n",
    "        print(\"NameNode is healthy.\")\n",
    "    else:\n",
    "        print(\"NameNode is not healthy.\")\n",
    "\n",
    "    if dn_status.ok:\n",
    "        print(\"DataNodes are healthy.\")\n",
    "    else:\n",
    "        print(\"DataNodes are not healthy.\")\n",
    "\n",
    "# Usage:\n",
    "nn_url = \"http://namenode:50070/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "dn_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=DataNodeStatus\"\n",
    "check_hadoop_health(nn_url, dn_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def list_hdfs_path(path):\n",
    "    command = f\"hdfs dfs -ls {path}\"\n",
    "    output = subprocess.check_output(command, shell=True).decode(\"utf-8\").strip()\n",
    "    files = output.split(\"\\n\")\n",
    "    \n",
    "    for file_info in files:\n",
    "        print(file_info)\n",
    "\n",
    "# Usage:\n",
    "path = \"/user/hadoop/data\"\n",
    "list_hdfs_path(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_data_node_storage(dn_url):\n",
    "    response = requests.get(dn_url)\n",
    "    data = response.json()\n",
    "\n",
    "    storage_reports = data[\"beans\"][0][\"StorageReport\"]\n",
    "    storage_reports.sort(key=lambda x: x[\"capacity\"], reverse=True)\n",
    "    \n",
    "    highest_capacity_node = storage_reports[0]\n",
    "    lowest_capacity_node = storage_reports[-1]\n",
    "\n",
    "    print(f\"Highest Capacity Node: {highest_capacity_node['name']}\")\n",
    "    print(f\"Capacity: {highest_capacity_node['capacity']}\")\n",
    "    \n",
    "    print(f\"\\nLowest Capacity Node: {lowest_capacity_node['name']}\")\n",
    "    print(f\"Capacity: {lowest_capacity_node['capacity']}\")\n",
    "\n",
    "# Usage:\n",
    "dn_url = \"http://datanode:50075/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-UndefinedStorageId\"\n",
    "analyze_data_node_storage(dn_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def submit_hadoop_job(job_url, job_params):\n",
    "    response = requests.post(job_url, json=job_params)\n",
    "    job_id = response.json()[\"jobId\"]\n",
    "    print(f\"Submitted Hadoop job with ID: {job_id}\")\n",
    "\n",
    "    progress_url = f\"{job_url}/{job_id}/progress\"\n",
    "    while True:\n",
    "        progress = requests.get(progress_url).json()\n",
    "        if progress[\"state\"] == \"SUCCEEDED\":\n",
    "            break\n",
    "        elif progress[\"state\"] == \"FAILED\":\n",
    "            print(\"Job execution failed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job progress: {progress['progress']}\")\n",
    "    \n",
    "    if progress[\"state\"] == \"SUCCEEDED\":\n",
    "        output_url = f\"{job_url}/{job_id}/output\"\n",
    "        response = requests.get(output_url)\n",
    "        output = response.json()[\"output\"]\n",
    "        print(f\"\\nFinal output: {output}\")\n",
    "\n",
    "# Usage:\n",
    "job_url = \"http://resourcemanager:8088/ws/v1/cluster/apps\"\n",
    "job_params = {\n",
    "    \"name\": \"WordCountJob\",\n",
    "    \"jar\": \"wordcount.jar\",\n",
    "    \"input\": \"/user/hadoop/input\",\n",
    "    \"output\": \"/user/hadoop/output\"\n",
    "}\n",
    "submit_hadoop_job(job_url, job_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def submit_hadoop_job_with_resources(job_url, job_params, resources):\n",
    "    response = requests.post(job_url, json=job_params)\n",
    "    job_id = response.json()[\"jobId\"]\n",
    "    print(f\"Submitted Hadoop job with ID: {job_id}\")\n",
    "\n",
    "    resource_url = f\"{job_url}/{job_id}/resource\"\n",
    "    for resource in resources:\n",
    "        requests.post(resource_url, json=resource)\n",
    "    \n",
    "    progress_url = f\"{job_url}/{job_id}/progress\"\n",
    "    while True:\n",
    "        progress = requests.get(progress_url).json()\n",
    "        if progress[\"state\"] == \"SUCCEEDED\":\n",
    "            break\n",
    "        elif progress[\"state\"] == \"FAILED\":\n",
    "            print(\"Job execution failed.\")\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Job progress: {progress['progress']}\")\n",
    "            \n",
    "        resource_usage_url = f\"{job_url}/{job_id}/resourceusage\"\n",
    "        resource_usage = requests.get(resource_usage_url).json()\n",
    "        print(f\"Resource usage: {resource_usage}\")\n",
    "    \n",
    "    if progress[\"state\"] == \"SUCCEEDED\":\n",
    "        output_url = f\"{job_url}/{job_id}/output\"\n",
    "        response = requests.get(output_url)\n",
    "        output = response.json()[\"output\"]\n",
    "        print(f\"\\nFinal output: {output}\")\n",
    "\n",
    "# Usage:\n",
    "job_url = \"http://resourcemanager:8088/ws/v1/cluster/apps\"\n",
    "job_params = {\n",
    "    \"name\": \"WordCountJob\",\n",
    "    \"jar\": \"wordcount.jar\",\n",
    "    \"input\": \"/user/hadoop/input\",\n",
    "    \"output\": \"/user/hadoop/output\"\n",
    "}\n",
    "resources = [\n",
    "    {\"resource\": \"memory\", \"amount\": \"4096\"},\n",
    "    {\"resource\": \"vcores\", \"amount\": \"2\"}\n",
    "]\n",
    "submit_hadoop_job_with_resources(job_url, job_params, resources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def run_mapreduce_job(input_file, split_size):\n",
    "    command = f\"hadoop jar mapreduce.jar JobName {input_file} output_dir -D mapreduce.input.fileinputformat.split.maxsize={split_size}\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "# Usage:\n",
    "input_file = \"large_input_file.txt\"\n",
    "split_sizes = [64, 128, 256, 512]\n",
    "\n",
    "for split_size in split_sizes:\n",
    "    print(f\"Running MapReduce job with split size: {split_size}\")\n",
    "    run_mapreduce_job(input_file, split_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
